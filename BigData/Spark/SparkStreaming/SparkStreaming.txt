--------------------------------------------------core concepts------------------------------------------
******************************StreamingContext******************************
// The batch interval must be set based on the latency requirements of your application and available cluster resources.
  def this(sparkContext: SparkContext, batchDuration: Duration) = {
    this(sparkContext, null, batchDuration)
  }
  
  def this(conf: SparkConf, batchDuration: Duration) = {
    this(StreamingContext.createNewSparkContext(conf), null, batchDuration)
  }

After a context is defined, you have to do the following.

1. Define the input sources by creating input DStreams.
2. Define the streaming computations by applying transformation and output operations to DStreams.
3. Start receiving data and processing it using streamingContext.start().
4. Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().
5. The processing can be manually stopped using streamingContext.stop().

Points to remember:
Once a context has been started, no new streaming computations can be set up or added to it.
Once a context has been stopped, it cannot be restarted.
Only one StreamingContext can be active in a JVM at the same time.
stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.
A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.
************************************************************************

*******************Discretized Streams (DStreams) - 使Stream其离散成一批批的RDD****************
 It represents a continuous stream of data, 
 either the input data stream received from source, 
 or the processed data stream generated by transforming the input stream. 
 Internally, a DStream is represented by a continuous series of RDDs.
 DStream ---- RDD @time 1 ---- RDD @time 2 ---- RDD @time 3 ---- RDD @time 4 ---- .....
 ***********************************************************************************************
 
 *********************************Input DStreams and Receivers**********************************
 Input DStreams are DStreams representing the stream of input data received from streaming sources.
 Every input DStream (except file stream(just read it, so Receiver is not needed))
 is associated with a Receiver object which receives the data from a source and stores it in Spark’s memory for processing.
 
 Spark Streaming provides two categories of built-in streaming sources.
 * Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.
 * Advanced sources: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.
 
 Points to remember
 * When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. 
 If you are using an input DStream based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. 
 Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to run.
 * Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. Otherwise the system will receive data, but not be able to process it.
 ***********************************************************************************************
 
 *********************************Input DStreams and Receivers***************************************
 Similar to that of RDDs, transformations allow the data from the input DStream to be modified.
 like map(func), flatMap(func), filter(func)....
 ******************************************************************************************************
 
 *********************************Output Operations on DStreams***************************************
 Output operations allow DStream’s data to be pushed out to external systems like a database or a file systems. 
 Since the output operations actually allow the transformed data to be consumed by external systems, they trigger the actual execution of all the DStream transformations(RDD actions)
 ******************************************************************************************************
 
 --------------------------------------------------end core concepts------------------------------------------
 
 *********************************updateStateByKey*********************************
	// 如果使用了stateful，必须要设置checkpoint来存储state信息
	// 把checkpoint存储目录设置为当前目录
	// 生产环境中建议把checkpoint目录设置到HDFS之类的可靠文件系统中
	ssc.checkpoint(".")

	val lines = ssc.socketTextStream("localhost", 6789)
	val wordMap = lines.flatMap(_.split(" ")).map((_, 1))

	// Return a new "state" DStream where the state for each key is updated by applying
	// the given function on the previous state of the key and the new values of each key.
	// If `this` function returns None, then corresponding state key-value pair will be eliminated.
	val result = wordMap.updateStateByKey[Int](updateFunction _)
 
     /**
      * @param curValues the current key's value seq. eg. word_a: [1, 1, 1, 1, 1, 1,...]
      * @param preValues the previous key's value. eg. word_a: pre_count
      * @return
      */
    def updateFunction(curValues: Seq[Int], preValues: Option[Int]): Option[Int] = {
        Some(curValues.sum + preValues.getOrElse(0))
    }
 ****************************************************************************************
	
 *********************************foreachRDD*********************************
        /*-- save data to MySQL --*/
        /* bad example 1 */
//        dstream.foreachRDD { rdd =>
//            val connection = createNewConnection()  // executed at the driver
//            rdd.foreach { record =>
//                connection.send(record) // executed at the worker
//            }
//        }
        //This is incorrect as this requires the connection object to be serialized and sent from the driver to the worker.
        // Such connection objects are rarely transferable across machines. This error may manifest as serialization errors (connection object not serializable)

        /* bad example 2 */
//        dstream.foreachRDD { rdd =>
//            rdd.foreach { record =>
//                val connection = createNewConnection()
//                connection.send(record)
//                connection.close()
//            }
//        }
        // creating and destroying a connection object for each record can incur unnecessarily high overheads and can significantly reduce the overall throughput of the system.

        /* not too bad example 3 */
//        dstream.foreachRDD { rdd =>
//            rdd.foreachPartition { partitionOfRecords =>
//                val connection = createNewConnection()
//                partitionOfRecords.foreach(record => connection.send(record))
//                connection.close()
//            }
//        }

        /* optimized example 4 */
        dstream.foreachRDD { rdd =>
            rdd.foreachPartition { partitionOfRecords =>
                // ConnectionPool is a static, lazily initialized pool of connections
                val connection = ConnectionPool.getConnection()
                partitionOfRecords.foreach(record => connection.send(record))
                ConnectionPool.returnConnection(connection)  // return to the pool for future reuse
            }
        } 
***************************************************************************************************